{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to HTML and BeautifulSoup for NLP Preprocessing\n",
    "\n",
    "Welcome to this introductory guide on using HTML (HyperText Markup Language) and BeautifulSoup for preprocessing webpages in Natural Language Processing (NLP) applications. In the vast expanse of the internet, a significant amount of information is stored in the form of webpages. These webpages are primarily written in HTML, which is the standard language for creating and designing web content.\n",
    "\n",
    "## Understanding HTML\n",
    "\n",
    "Before diving into the specifics of BeautifulSoup and its applications in NLP, it's crucial to understand the basics of HTML:\n",
    "\n",
    "- HTML Structure: HTML documents are composed of a series of elements, represented by tags (like `<p>` for paragraphs, `<h1>` for main headings, `<div>` for divisions/sections, etc.). These elements structure the webpage and define its content.\n",
    "\n",
    "- Nested Elements: HTML elements can be nested within each other, creating a parent-child relationship in the webpage's structure. This hierarchical structure is key to navigating and extracting specific data from webpages.\n",
    "\n",
    "- Attributes: HTML elements can have attributes (like `class`, `id`, `href`, etc.) that provide additional information about the element, often used for styling or identifying elements.\n",
    "\n",
    "## Comparing HTML and Markdown\n",
    "\n",
    "HTML (HyperText Markup Language) and Markdown are both markup languages used for creating formatted text, but they serve different purposes and exhibit distinct characteristics. By comparing a sample document in both HTML and Markdown side by side, we can observe these differences in complexity, flexibility, and ease of use, highlighting the unique advantages each language brings to text formatting and web content creation.\n",
    "\n",
    "- [Sample HTML document](./sample_html_doc.html)\n",
    "- [Sample MD document](./sample_md_doc.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to BeautifulSoup\n",
    "\n",
    "With the knowledge of HTML in hand, we can leverage [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), a powerful Python library, to parse and extract information from HTML documents:\n",
    "\n",
    "- Parsing HTML: BeautifulSoup can read and parse HTML content, allowing you to navigate the structure of a webpage programmatically. It's like giving you the ability to read and understand webpages as a browser does, but with the intention of data extraction and manipulation.\n",
    "\n",
    "- Locating Elements: Using BeautifulSoup, you can find specific elements, extract data, modify the HTML, or even build an entirely new HTML document. It provides methods to search for elements by tags, attributes, text content, and more.\n",
    "\n",
    "- NLP Applications: In NLP, preprocessing is a critical step. When working with text data from webpages, BeautifulSoup enables you to clean and prepare this data. This includes tasks like extracting readable text from HTML, removing irrelevant content (like navigation bars, ads, footers), and structuring the text data for NLP models.\n",
    "\n",
    "In the following section, we'll use the sample HTML document and demonstrate how to apply BeautifulSoup for various preprocessing tasks, setting the stage for more advanced NLP applications. This hands-on approach will give you the practical skills needed to harness the power of web scraping in conjunction with NLP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Try running the cell below while your virtual environment is activated. If you get a ModuleNotFound error, install using `python -m pip install beautifulsoup4`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup object\n",
    "\n",
    "At the heart of BeautifulSoup is the \"soup\" object, which represents the parsed HTML (or XML) document and provides a rich set of methods to navigate, search, and modify the parse tree.\n",
    "\n",
    "When you create a BeautifulSoup object, you essentially convert a string of HTML or XML into a complex tree of Python objects. This conversion process makes it possible to interact with the HTML elements programmatically, as if they were typical Python objects. Here are some key features and capabilities of the soup object:\n",
    "\n",
    "- Parsing HTML/XML: BeautifulSoup can parse content from various sources - such as from a string or a file - and supports different parsers (like html.parser, lxml, etc.), offering flexibility in handling various types of HTML/XML inputs.\n",
    "\n",
    "- Navigating the Tree: The soup object allows you to navigate the parse tree using tag names, which can be as straightforward as accessing Python attributes. It's like traversing a hierarchical structure of elements in a webpage.\n",
    "\n",
    "- Searching the Document: With methods like `find()` and `find_all()`, the soup object enables you to locate elements based on tags, attributes, text content, or even CSS selectors. This powerful searching capability is essential for extracting specific data from complex web pages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "\n",
      "<html>\n",
      "<head>\n",
      "<title>DPAA02 - HTML Introduction</title>\n",
      "</head>\n",
      "<body>\n",
      "<img alt=\"HTML5 Logo\" src=\"https://www.w3.org/html/logo/downloads/HTML5_Badge_128.png\"/>\n",
      "<h1 id=\"main-header\">Introduction to HTML Cleaning</h1>\n",
      "<p class=\"intro\">\n",
      "      This is a sample HTML document used to demonstrate the process of HTML\n",
      "      cleaning using BeautifulSoup.\n",
      "    </p>\n",
      "<p class=\"intro\">\n",
      "      This is a sample HTML document used to demonstrate the process of HTML\n",
      "      cleaning using BeautifulSoup.\n",
      "    </p>\n",
      "<h2 class=\"sub-header\">Why Clean HTML?</h2>\n",
      "<p class=\"description\">\n",
      "      Cleaning HTML is essential for web scraping, as it helps in extracting\n",
      "      useful information by removing unnecessary tags and formatting.\n",
      "    </p>\n",
      "<p class=\"conclusion\">\n",
      "      HTML cleaning can greatly improve the efficiency and accuracy of data\n",
      "      extraction in web scraping.\n",
      "    </p>\n",
      "<h3 class=\"table-header\">Sample Table</h3>\n",
      "<table border=\"1\" class=\"data-table\">\n",
      "<tr>\n",
      "<th>Item</th>\n",
      "<th>Quantity</th>\n",
      "<th>Price</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Apples</td>\n",
      "<td>4</td>\n",
      "<td>$1.50</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Oranges</td>\n",
      "<td>3</td>\n",
      "<td>$2.00</td>\n",
      "</tr>\n",
      "</table>\n",
      "<h3 class=\"table-header\">Another Table</h3>\n",
      "<table border=\"1\" class=\"info-table\">\n",
      "<tr>\n",
      "<th>Name</th>\n",
      "<th>Age</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Alice</td>\n",
      "<td>30</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Bob</td>\n",
      "<td>35</td>\n",
      "</tr>\n",
      "</table>\n",
      "<p>\n",
      "      For more information, visit the wikipedia article on\n",
      "      <a href=\"https://en.wikipedia.org/wiki/HTML\" target=\"_blank\">HTML</a>.\n",
      "    </p>\n",
      "<p>\n",
      "      The\n",
      "      <a href=\"https://www.w3schools.com/html/html_intro.asp\" target=\"_blank\">\n",
      "        W3Schools HTML tutorial\n",
      "      </a>\n",
      "      is also an excellent resource.\n",
      "    </p>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use context manager to read the sample document and initialise the soup object with `html.parser`\n",
    "with open('sample_html_doc.html', 'r') as file:\n",
    "    soup = BeautifulSoup(file, 'html.parser')\n",
    "    \n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching the document\n",
    "\n",
    "In BeautifulSoup, `find` and `find_all` are two commonly used methods for searching elements in an HTML or XML document, but they serve different purposes:\n",
    "\n",
    "- `find` Method: The `find` method is used to retrieve the first occurrence of a tag or element that matches the specified criteria. It's useful when you're interested in only the first instance of an element, such as the first paragraph or the first header in a document. For example, `soup.find('p')` will return the first `<p> `tag it encounters in the HTML content.\n",
    "\n",
    "- `find_all` Method: In contrast, `find_all` collects all elements in the document that match the specified criteria and returns them as a list. This method is ideal when you need to gather all instances of a particular element, such as all links or all images on a page. For instance, `soup.find_all('p')` will return a list of all `<p>` tags present in the document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a sample HTML document used to demonstrate the process of HTML\\n      cleaning using BeautifulSoup.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the first paragraph using .find\n",
    "soup.find('p').text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"intro\">\n",
       "       This is a sample HTML document used to demonstrate the process of HTML\n",
       "       cleaning using BeautifulSoup.\n",
       "     </p>,\n",
       " <p class=\"intro\">\n",
       "       This is a sample HTML document used to demonstrate the process of HTML\n",
       "       cleaning using BeautifulSoup.\n",
       "     </p>,\n",
       " <p class=\"description\">\n",
       "       Cleaning HTML is essential for web scraping, as it helps in extracting\n",
       "       useful information by removing unnecessary tags and formatting.\n",
       "     </p>,\n",
       " <p class=\"conclusion\">\n",
       "       HTML cleaning can greatly improve the efficiency and accuracy of data\n",
       "       extraction in web scraping.\n",
       "     </p>,\n",
       " <p>\n",
       "       For more information, visit the wikipedia article on\n",
       "       <a href=\"https://en.wikipedia.org/wiki/HTML\" target=\"_blank\">HTML</a>.\n",
       "     </p>,\n",
       " <p>\n",
       "       The\n",
       "       <a href=\"https://www.w3schools.com/html/html_intro.asp\" target=\"_blank\">\n",
       "         W3Schools HTML tutorial\n",
       "       </a>\n",
       "       is also an excellent resource.\n",
       "     </p>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find all paragraphs using .find_all\n",
    "soup.find_all('p')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Text from HTML Elements in BeautifulSoup\n",
    "\n",
    "After locating an HTML element using BeautifulSoup, a common task is to extract the textual content from that element. BeautifulSoup makes this process straightforward, offering a couple of key ways to access the text within an HTML element:\n",
    "\n",
    "- The `.text` Attribute: Once you have identified an HTML element (or elements) using methods like `find` or `find_all`, you can use the `.text` attribute to extract all the text content within that element. This attribute aggregates all the text in an element and its descendants, returning it as a single string. For example, after finding a paragraph with `p = soup.find('p')`, you can get its text with `p.text`.\n",
    "\n",
    "- The `.get_text()` Method: An alternative to the `.text` attribute is the `.get_text()` method. This method is more versatile, allowing you to specify a separator between the texts of different descendants and an option to strip extra whitespaces. For instance, if you have a `<div>` containing multiple `<p>` tags, `div.get_text(\"|\")` will extract the text from each `<p>` tag and join them using the `|` character as a separator.\n",
    "\n",
    "It's important to note that both `.text` and `.get_text()` will strip away any HTML tags and return only the human-readable text content. This makes them incredibly useful for NLP tasks where the goal is to extract information rather than HTML structure.\n",
    "\n",
    "In summary, extracting text from HTML elements is a frequent requirement in web scraping and data extraction, and BeautifulSoup provides simple yet powerful tools to accomplish this task effectively. Whether you're scraping paragraphs, headers, or other textual elements, these methods offer a clean and efficient way to access the content you need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      This is a sample HTML document used to demonstrate the process of HTML\n",
      "      cleaning using BeautifulSoup.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Get the text of the first paragraph using .text\n",
    "print(soup.find('p').text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample HTML document used to demonstrate the process of HTML\n",
      "      cleaning using BeautifulSoup.\n"
     ]
    }
   ],
   "source": [
    "print(soup.find('p').get_text(strip=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample HTML document used to demonstrate the process of HTML\n",
      "      cleaning using BeautifulSoup.\n",
      "This is a sample HTML document used to demonstrate the process of HTML\n",
      "      cleaning using BeautifulSoup.\n",
      "Cleaning HTML is essential for web scraping, as it helps in extracting\n",
      "      useful information by removing unnecessary tags and formatting.\n",
      "HTML cleaning can greatly improve the efficiency and accuracy of data\n",
      "      extraction in web scraping.\n",
      "For more information, visit the wikipedia article onHTML.\n",
      "TheW3Schools HTML tutorialis also an excellent resource.\n"
     ]
    }
   ],
   "source": [
    "# Get the texts of all the paragraphs (you have to iterate over the list)\n",
    "paragraphs = soup.find_all('p')\n",
    "# print(paragraphs)\n",
    "\n",
    "for paragraph in paragraphs:\n",
    "    print(paragraph.get_text(strip=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using find and find_all with Additional Attributes in BeautifulSoup\n",
    "\n",
    "In BeautifulSoup, the `find` and `find_all` methods are not limited to simple tag searches. They can be significantly more powerful when used with additional attributes, enabling more precise and specific element selection based on their attributes in the HTML structure.\n",
    "\n",
    "- Attribute Filtering: You can pass additional attributes to find and find_all methods to filter elements by their attributes. For example, if you want to find all `<p>` tags with a class `intro`, you can use `soup.find_all(\"p\", {\"class\": \"intro\"})`.\n",
    "\n",
    "- Multiple Attributes: These methods also allow for filtering using multiple attributes simultaneously. For instance, `soup.find_all(\"div\", {\"class\": \"container\", \"id\": \"main\"})` will find all `<div>` elements with a class of `container` and an ID of `main`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p class=\"description\">\n",
       "      Cleaning HTML is essential for web scraping, as it helps in extracting\n",
       "      useful information by removing unnecessary tags and formatting.\n",
       "    </p>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the paragraph with class=\"description\"\n",
    "soup.find('p', {'class': 'description'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Links from `<a>` Tags Using BeautifulSoup\n",
    "\n",
    "Hyperlinks are fundamental components of the web, interconnecting various resources and web pages. In web scraping, extracting these links often involves targeting `<a>` (anchor) tags in HTML, which traditionally define hyperlinks. BeautifulSoup simplifies this process, allowing for efficient extraction of URLs embedded within these tags. We often have to extract links from a document in NLP applications.\n",
    "\n",
    "#### Understanding `<a>` Tags\n",
    "\n",
    "An `<a>` tag in HTML typically includes an `href` attribute, which holds the URL the link points to.\n",
    "It may also contain other attributes and text, which provide additional context or display information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"https://en.wikipedia.org/wiki/HTML\" target=\"_blank\">HTML</a>, <a href=\"https://www.w3schools.com/html/html_intro.asp\" target=\"_blank\">\n",
      "        W3Schools HTML tutorial\n",
      "      </a>]\n"
     ]
    }
   ],
   "source": [
    "# First, use find_all to retrieve all the <a> tags from the soup object.\n",
    "all_a_tags = soup.find_all('a')\n",
    "print(all_a_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the list of <a> tags and use the .get('href') method to extract the URL from each tag's href attribute.\n",
    "all_a_tags[0].get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, you can also extract the text associated with each hyperlink to understand what the link represents.\n",
    "all_a_tags[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting URLs and Link Text together with Link Text as key and URL as value\n",
    "for item in all_a_tags:\n",
    "    url = item.get('href')\n",
    "    text = item.text.strip()\n",
    "    print(f\"{text}: {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Tabular Data from HTML using BeautifulSoup and Pandas\n",
    "\n",
    "One of the most common tasks in web scraping and data extraction is retrieving information stored in tables within HTML documents. HTML tables, marked up with `<table>` (overall table), `<tr>` (each table row), `<th>` (table header elements), and `<td>` (table data elements) tags, often contain structured data that are ideal for analysis. However, extracting this data and converting it into a usable format like a Pandas DataFrame requires a careful approach.\n",
    "\n",
    "In this section, we'll explore two powerful methods for extracting tabular data from HTML:\n",
    "\n",
    "- Using `BeautifulSoup`: This method involves parsing the HTML content with BeautifulSoup to navigate and extract the table data. It provides a high level of control, allowing for the handling of complex or irregular table structures. We'll manually read the table rows and columns, extract the text, and then structure this data into a DataFrame.\n",
    "\n",
    "- Using Pandas `read_html`: Pandas offers a convenient function read_html that automatically parses and converts HTML tables into DataFrame objects. This method is incredibly efficient and requires minimal code, making it ideal for straightforward tables. However, it may not always be suitable for tables with complex structures or non-standard formatting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the first table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data into a list of lists\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas.read_html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading HTML pages from the internet\n",
    "\n",
    "So far, we have been working with a local HTML file (`*.html`). But we can use the `requests` library to get a page from the Internet using the `.get` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use requests.get and pass the url you want to obtain\n",
    "r = requests.get(r\"https://en.wikipedia.org/wiki/HTML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now r.text will contain the html which we can process using beautiful soup.\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Scraping content from a Wikipedia page\n",
    "\n",
    "Find a Wikipedia article that you are interested in and scrape all the paragraphs from it.\n",
    "\n",
    "**Stretch**: Find an article with a large table (e.g. <https://en.wikipedia.org/wiki/List_of_highest-grossing_films>) and scrape the table using BeautifulSoup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml_q42023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
